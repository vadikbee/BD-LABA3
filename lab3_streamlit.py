# lab3_streamlit.py
import streamlit as st
import numpy as np
import base64
import time
import os
import json
from PIL import Image
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    import tensorflow.lite as tflite

# ----- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å—Ç–∏–ª–∏ -----
st.set_page_config(
    page_title="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–∞–±–æ—á–µ–∫",
    page_icon="ü¶ã",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- –§—É–Ω–∫—Ü–∏–∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è ---
@st.cache_data
def get_base64(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()

def set_background(jpg_file):
    try:
        bin_str = get_base64(jpg_file)
        page_bg_img = f'''
        <style>
        .stApp {{
            background-image: url("data:image/jpg;base64,{bin_str}");
            background-size: cover;
        }}
        </style>
        '''
        st.markdown(page_bg_img, unsafe_allow_html=True)
    except FileNotFoundError:
        st.warning(f"–§–∞–π–ª –¥–ª—è —Ñ–æ–Ω–∞ '{jpg_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")

@st.cache_resource
def load_trained_model():
    try:
        interpreter = tflite.Interpreter(model_path='butterfly_model_quant.tflite')
        interpreter.allocate_tensors()
        with open('class_indices.json', 'r') as f:
            class_indices = json.load(f)
        return interpreter, class_indices
    except Exception:
        return None, None

@st.cache_data
def load_data_info(train_path):
    try:
        if not os.path.exists(train_path):
            return None, None
        num_classes = len([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])
        num_train_images = sum([len(files) for r, d, files in os.walk(train_path)])
        return num_classes, num_train_images
    except Exception:
        return None, None

@st.cache_data
def load_evaluation_results(file_path='evaluation_results.json'):
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return None

# --- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
set_background('sakura.jpg')
st.title('ü¶ã –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–∏–¥–æ–≤ –±–∞–±–æ—á–µ–∫')
st.write('–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ3.')

interpreter, class_indices = load_trained_model()
height, width = 128, 128
st.sidebar.header("–ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
app_mode = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª:", ["–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"])

if app_mode == "–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö":
    st.header("1. –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
    num_classes, num_train_images = load_data_info('train_structured')
    if num_classes is not None:
        st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (–≤–∏–¥–æ–≤)", f"{num_classes} ü¶ã")

    st.divider()
    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Ç–æ–≥–æ–≤–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏")
    results = load_evaluation_results()
    if results:
        col1, col2 = st.columns(2)
        col1.metric("1. –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (Accuracy)", f"{results.get('accuracy', 0):.2%}")
        col2.metric("2. –û–±—â–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", f"{results.get('prediction_time', 0):.2f} —Å–µ–∫.")
    else:
        st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ `train_butterfly.py`.")

elif app_mode == "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ":
    st.header("2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ —Å–≤–æ–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é")
    if interpreter is None or class_indices is None:
        st.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã `butterfly_model_quant.tflite` –∏ `class_indices.json` —Å—É—â–µ—Å—Ç–≤—É—é—Ç.")
    else:
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–∞–±–æ—á–∫–∏...", type=["jpg", "jpeg", "png"])
        if uploaded_file:
            image = Image.open(uploaded_file).convert('RGB')
            col1, col2 = st.columns([1, 1])
            col1.image(image, caption='–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', use_container_width=True)
            if col2.button("–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å", use_container_width=True):
                with st.spinner("–ê–Ω–∞–ª–∏–∑..."):
                    img_array = np.array(image.resize((width, height)), dtype=np.float32) / 255.0
                    img_array = np.expand_dims(img_array, axis=0)
                    
                    input_details = interpreter.get_input_details()
                    output_details = interpreter.get_output_details()
                    interpreter.set_tensor(input_details[0]['index'], img_array)
                    
                    start_time = time.time()
                    interpreter.invoke()
                    end_time = time.time()

                    prediction = interpreter.get_tensor(output_details[0]['index'])
                    pred_index = np.argmax(prediction)
                    class_labels = {v: k for k, v in class_indices.items()}
                    pred_name = class_labels[pred_index]
                    confidence = np.max(prediction)

                    col2.success(f"### –í–∏–¥: **{pred_name}**")
                    col2.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** `{confidence:.2%}`")
                    col2.metric("–í—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", f"{(end_time - start_time) * 1000:.2f} –º—Å")